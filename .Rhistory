knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
tweets_df <- read.csv("C:/Users/kurts/Desktop/SentimentAnalysis/tweetsDF.csv")
# Remove duplicates
tweets_df <- tweets_df %>%
distinct()
missing_values <- colSums(is.na(tweets_df))
# Clean the text column
tweets_df$text <- tweets_df$text %>%
str_replace_all("http\\S+|www\\.\\S+", "") %>% # Remove URLs
str_replace_all("[^[:alnum:][:space:]]", "") %>% # Remove special characters
str_squish() # Remove extra whitespaces
print(head(tweets_df))
print(missing_values)
# Trend Analysis
install.packages("lubridate")
library(dplyr)
library(ggplot2)
library(lubridate)
tweets_df$created <- ymd_hms(tweets_df$created)
daily_trend <- tweets_df %>%
mutate(date = as_date(created)) %>%
group_by(date) %>%
summarise(tweet_count = n())
ggplot(daily_trend, aes(x = date, y = tweet_count)) +
geom_line(color = "black", linewidth = 1) +
labs(
title = "Daily Tweet Trend",
x = "Date",
y = "Number of Tweets"
) +
theme_minimal()
# Sentimental Analysis
install.packages("tidytext")
library(dplyr)
library(tidytext)
library(ggplot2)
bing_lexicon <- get_sentiments("bing")
tokenized_tweets <- tweets_df %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
options(repos = c(CRAN = "https://cloud.r-project.org"))
options(repos = c(CRAN = "https://cloud.r-project.org"))
packages <- c("rmarkdown", "knitr", "tidyverse")
install.packages(packages, dependencies = TRUE)
install.packages(packages, dependencies = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
tweets_df <- read.csv("C:/Users/kurts/Desktop/SentimentAnalysis/tweetsDF.csv")
install.packages(packages, dependencies = TRUE)
